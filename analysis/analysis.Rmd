---
title: "Bootstrapped estimation of D scores for individual participants"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# To do

- Add ridge plots for data_D_boots_iterations

# Overview 

IAT trial type D scores are calculated from an average of only 18 pairs of reaction times. This would be deemed as far too low anywhere else in the reaction time literature. The implications of this can be seen in how poorly estimated any one IAT D score is. We can observe this by bootstrapping reaction times for each participant and trial type.

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      cache.lazy=FALSE)
```

```{r}

# dependencies
library(tidyverse)
library(knitr)
library(kableExtra)
library(rsample)
library(broom)
library(purrr)
library(furrr)

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, round, digits = n_digits)
}

# run furrr:::future_map in parallel
plan(multiprocess)

# options
options(knitr.table.format = "html") # necessary configuration of tables

# disable scientific notation
options(scipen = 999) 

# get data 
data_iat_for_scoring_subset <- read_rds("../data/data_iat_for_scoring_subset.rds")

```

# Descriptives

```{r}

data_descriptives <- data_for_analysis %>%
  distinct(session_id, .keep_all = TRUE) 

data_descriptives %>%
  count(domain) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_descriptives %>%
  count(domain) %>% 
  summarize(total_n = sum(n),
            min_n_per_domain = min(n),
            max_n_per_domain = max(n),
            mean_n_per_domain = round(mean(n, na.rm = TRUE), 2),
            sd_n_per_domain = round(sd(n, na.rm = TRUE), 2)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_descriptives %>%
  summarize(min_age = round(min(age, na.rm = TRUE), 2),
            max_age = round(max(age, na.rm = TRUE), 2),
            mean_age = round(mean(age, na.rm = TRUE), 2),
            sd_age = round(sd(age, na.rm = TRUE), 2)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_descriptives %>%
  count(gender) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```
# Bootstrap 95% CIs on D scores

## Bootstrapping CIs

```{r}

# bootstrapping has a long execution time, so load saved values if they've already been calculated
if(file.exists("models/data_D_scores_with_bootstrapped_CIs.rds")) {
  
  data_D_scores_with_bootstrapped_CIs <- read_rds("models/data_D_scores_with_bootstrapped_CIs.rds")
  
} else {
  
  # n boots for all metrics
  nboots <- 10 # 2000
  
  # trim RTs>10000 ms
  data_trimmed <- data_iat_for_scoring_subset %>%
    filter(trial_latency <= 10000) 
  
  # create D scores
  data_D_scores <- data_trimmed %>%
    group_by(session_id) %>%
    summarise(Da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                sd(trial_latency[block_number %in% c(2, 5)]),
              Db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                sd(trial_latency[block_number %in% c(3, 6)])) %>% 
    mutate(D = (Da + Db)/2) %>%
    ungroup() %>%
    select(-Da, -Db) %>%
    round_df(3)
  
  # create resamples
  data_splits <- data_trimmed %>%
    group_by(session_id) %>%
    bootstraps(times = nboots)

  # function to apply to each resample
  calc_D <- function(split) {
    analysis(split) %>%
      group_by(session_id) %>%
      summarise(Da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                  sd(trial_latency[block_number %in% c(2, 5)]),
                Db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                  sd(trial_latency[block_number %in% c(3, 6)])) %>% 
      mutate(D = (Da + Db)/2) %>%
      ungroup() %>%
      select(-Da, -Db) %>%
      round_df(3)
  }
  
  # start timer
  start <- Sys.time()
  
  # apply to each bootstrap
  data_D_scores_with_bootstrapped_CIs <- data_splits %>%
    mutate(D_metrics = furrr::future_map(splits, calc_D)) %>%
    select(-splits) %>%
    unnest(D_metrics) %>%
    group_by(session_id) %>%
    dplyr::summarize(D_ci_lwr = quantile(D, 0.025, na.rm = TRUE),
                     D_ci_upr = quantile(D, 0.975, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(Effect = ifelse((D_ci_lwr < 0 & D_ci_upr < 0) |
                             (D_ci_lwr > 0 & D_ci_upr > 0), "Significant", "Non-signficant"),
           Effect_binary = ifelse(Effect == "Significant", TRUE, FALSE),
           D_ci_width = D_ci_upr - D_ci_lwr) %>%
    full_join(data_D_scores, by = "session_id") %>%
    left_join(distinct(select(data_trimmed, session_id, domain), .keep_all = TRUE), by = "session_id")
  
  # end timer
  end <- Sys.time()
  
  # calculate total time
  end - start

  # save to disk
  write_rds(data_D_scores_with_bootstrapped_CIs, file = "models/data_D_scores_with_bootstrapped_CIs.rds")
  
}

```

## Plots

```{r}

# plot
ggplot(data_D_scores_with_bootstrapped_CIs) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = session_id, ymin = D_ci_lwr, ymax = D_ci_upr, color = Effect), 
                 alpha = 0.3) + 
  geom_point(aes(session_id, D_median, color = Effect), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participants ranked by D score") +
  ylab("IAT D score")

# separated by domains
ggplot(data_D_scores_with_bootstrapped_CIs) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = session_id, ymin = D_ci_lwr, ymax = D_ci_upr, color = Effect),
                 alpha = 0.3) +
  #geom_point(aes(session_id, D, color = Effect)) +
  geom_point(aes(session_id, D_median, color = Effect), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participant") +
  ylab("IAT D score") +
  facet_wrap(~domain)

data_D_scores_with_bootstrapped_CIs %>%
  group_by(domain) %>%
  summarize(median_half_ci_width = round(median(D_ci_width)/2, 2)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

trimmed_emprirical_range <- data_D_scores_with_bootstrapped_CIs %>%
  dplyr::summarize(percentile_025 = quantile(D_median, 0.025, na.rm = TRUE),
                   percentile_975 = quantile(D_median, 0.975, na.rm = TRUE)) %>%
  mutate(range_empirical_95_percent = percentile_975 - percentile_025) 

```

## Proportion of participants demonstrating a significant bias

`r (1 - round(mean(data_D_scores_with_bootstrapped_CIs$Effect_binary), 3))*100`% of D scores are not signfiicant from zero. So, while they might appear to be relatively large (e.g., D = 0.5), their CI does not exclude zero. Put another way, if we treat the zero point as meaningful, we have insufficient evidence to say whether a given D score represents an IAT effect in `r (1 - round(mean(data_D_scores_with_bootstrapped_CIs$Effect_binary), 3))*100`% of cases in this large sample (`r nrow(data_D_scores_with_bootstrapped_CIs)/4` participants, `r nrow(data_D_scores_with_bootstrapped_CIs)` total D scores)).

## Estimation precision

Of course, while treating the zero point as meanining is common, it has been argued to problematic (i.e., by giving rise to false conclusions about the interpretations of the effect, see REF). While the previous analysis represents a useful illustration, it may be more meaningful to consider the precision of estimation of D scores, agnostic to an arbitrary cut-off point. 

The width of a D score Confidence intervals was found to be wide: M = `r round(mean(data_D_scores_with_bootstrapped_CIs$D_ci_width), 2)`, SD = `r round(sd(data_D_scores_with_bootstrapped_CIs$D_ci_width), 2)`, Median = `r round(median(data_D_scores_with_bootstrapped_CIs$D_ci_width), 2)`, MAD = `r round(mad(data_D_scores_with_bootstrapped_CIs$D_ci_width), 2)`. Results in the tables above suggest that it doesn't vary much by trial type or domain. As such, when an individual demonstrates a D score of X, we can more accurately say their D score lies in the range of X Â± a median of `r round(median(data_D_scores_with_bootstrapped_CIs$D_ci_width)/2, 2)`. 

While the minimum observed D scores was `r round(min(data_D_scores_with_bootstrapped_CIs$D_median), 2)` and max was `r round(max(data_D_scores_with_bootstrapped_CIs$D_median), 2)`, the outlier scores are clearly visible (see figure XXX). It is therefore likely to be more meaningful to note that 95% of D scores lie within the narrower range of `r round(as.numeric(trimmed_emprirical_range$percentile_025), 2)` to `r round(as.numeric(trimmed_emprirical_range$percentile_975), 2)`. It is useful to contextualise the precision of the estimation of a given IAT D score within this total observed ranged of D scores between participants. Specifically, the median CI width noted above represents `r round(median(data_D_scores_with_bootstrapped_CIs$D_ci_width)/as.numeric(trimmed_emprirical_range$range_empirical_95_percent)*100, 1)`% of the (95% trimmed) observed range of D scores. That is to say, the uncertainty around a given D score representst the majority of the total range of D scores: even knowing an individual's observed D score (e.g., moderately pro-white/anti-black), their 'true' D score may lie almost anywhere else on the range of possible values (e.g., from very pro-white/anti-black to very anti-white/pro-black). An individual IAT effect is therefore quite a poor measure for individual use.

This can also be examined another way by posing the question 'what proportion of D scores can you tell apart from one another?' That is, is the probability that a given D score lies outside the CI of all the other D scores' CIs. For simplicity of implementation, this analysis compares all D scores against all confidence intervals. It is therefore slightly biased by comparing a D score against its own CI as well as all others. However, given the large number of comparisons (i.e., sample size: i.e., `r nrow(data_D_scores_with_bootstrapped_CIs)` total D scores) this bias is very slight. 95% CIs of this probability value are bootstrapped via case removal and the percentile method using 2000 resamples. The median bootstrapped probability is reported as the estimate for the sake of robustness.

```{r}

boots <- bootstraps(data_D_scores_with_bootstrapped_CIs, times = 2000)

# helper function to apply workflow to each resample
helper_function <- function(split) {

  estimate <- analysis(split)$D_median
  ci_lower <- analysis(split)$D_ci_lwr
  ci_upper <- analysis(split)$D_ci_upr
  
  n_estimate <- length(estimate)
  n_ci_lower <- length(ci_lower)
  n_ci_upper <- length(ci_upper)
  
  r_estimate <- sum(rank(c(estimate, ci_lower))[1:n_estimate])
  r_ci_upper <- sum(rank(c(ci_upper, estimate))[1:n_ci_upper])
  
  prob_estimate_inferior_to_ci_lower <- 1 - (r_estimate / n_estimate - (n_estimate + 1) / 2) / n_ci_lower
  prob_estimate_superior_to_ci_upper <- 1 - (r_ci_upper / n_ci_upper - (n_ci_upper + 1) / 2) / n_estimate
  
  percent_estimates_inside_cis <- 1 - (prob_estimate_inferior_to_ci_lower + prob_estimate_superior_to_ci_upper)
  
  return(percent_estimates_inside_cis)
  
}

# apply to each bootstrap
boot_probabilities <- boots %>% 
  mutate(percent_estimates_inside_cis = furrr::future_map(splits, helper_function)) %>% 
  unnest(percent_estimates_inside_cis) %>%
  select(-splits)

# find CIs using percentile method
probability_estimates <- boot_probabilities %>% 
  summarize(median   = quantile(percent_estimates_inside_cis, 0.500),
            ci_lower = quantile(percent_estimates_inside_cis, 0.025),
            ci_upper = quantile(percent_estimates_inside_cis, 0.975)) %>%
  round_df(3)

```

It is not possible to differentiate between two randomly selected D scores in `r round(probability_estimates$median*100, 1)`% (95% CI [`r round(probability_estimates$ci_lower*100, 1)`, `r round(probability_estimates$ci_upper*100, 1)`]) of cases. This provides additional evidence that IRAP's individual level precision and therefore clinical utility is low.


