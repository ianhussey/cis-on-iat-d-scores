---
title: "Bootstrapped estimation of D scores for individual participants"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# To do

- Add ridge plots for data_D_boots_iterations

# Overview 

IAT trial type D scores are calculated from an average of only 18 pairs of reaction times. This would be deemed as far too low anywhere else in the reaction time literature. The implications of this can be seen in how poorly estimated any one IAT D score is. We can observe this by bootstrapping reaction times for each participant and trial type.

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      cache.lazy=FALSE)
```

```{r}

# dependencies
library(tidyverse)
library(knitr)
library(kableExtra)
library(rsample)
library(broom)
library(purrr)
library(furrr)

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, round, digits = n_digits)
}

# run furrr:::future_map in parallel
future::plan(multiprocess)
options(future.globals.maxSize = 3000 * 1024^2)

# options
options(knitr.table.format = "html") # necessary configuration of tables

# disable scientific notation
options(scipen = 999) 

# get data 
data_iat_for_scoring_subset <- read_rds("../data/data_iat_for_scoring_subset.rds")

```

# Descriptives

```{r}

# data_descriptives <- data_for_analysis %>%
#   distinct(session_id, .keep_all = TRUE) 
# 
# data_descriptives %>%
#   count(domain) %>% 
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
# 
# data_descriptives %>%
#   count(domain) %>% 
#   summarize(total_n = sum(n),
#             min_n_per_domain = min(n),
#             max_n_per_domain = max(n),
#             mean_n_per_domain = round(mean(n, na.rm = TRUE), 2),
#             sd_n_per_domain = round(sd(n, na.rm = TRUE), 2)) %>% 
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
# 
# data_descriptives %>%
#   summarize(min_age = round(min(age, na.rm = TRUE), 2),
#             max_age = round(max(age, na.rm = TRUE), 2),
#             mean_age = round(mean(age, na.rm = TRUE), 2),
#             sd_age = round(sd(age, na.rm = TRUE), 2)) %>% 
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
# 
# data_descriptives %>%
#   count(gender) %>% 
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Bootstrap 95% CIs on D scores

## Bootstrapping CIs

circa 5 hours runtime

```{r}

# bootstrapping has a long execution time, so load saved values if they've already been calculated
if(file.exists("models/data_estimates_with_CIs.rds")) {
  
  data_estimates_with_CIs <- read_rds("models/data_estimates_with_CIs.rds")
  
} else {
  
  # n boots for all metrics
  nboots <- 2000 
  
  # trim RTs>10000 ms
  data_trimmed <- data_iat_for_scoring_subset %>%
    filter(trial_latency <= 10000) 
  
  # create D scores
  data_D_scores <- data_trimmed %>%
    group_by(session_id) %>%
    summarise(Da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                sd(trial_latency[block_number %in% c(2, 5)]),
              Db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                sd(trial_latency[block_number %in% c(3, 6)])) %>% 
    mutate(D = (Da + Db)/2) %>%
    ungroup() %>%
    select(-Da, -Db) %>%
    round_df(3)
  
  # create cohens d scores
  data_d_scores <- data_trimmed %>%
    group_by(session_id) %>%
    summarise(da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                sqrt((sd(trial_latency[block_number == 5])^2 + sd(trial_latency[block_number == 2])^2)/2),
              db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                sqrt((sd(trial_latency[block_number == 6])^2 + sd(trial_latency[block_number == 3])^2)/2)) %>%
    mutate(d = (da + db)/2) %>%
    ungroup() %>%
    select(-da, -db) %>%
    round_df(3)

  # function to apply to each resample
  calc_D <- function(split) {
    analysis(split) %>%
      group_by(session_id) %>%
      summarise(Da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                  sd(trial_latency[block_number %in% c(2, 5)]),
                Db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                  sd(trial_latency[block_number %in% c(3, 6)])) %>% 
      mutate(D = (Da + Db)/2) %>%
      ungroup() %>%
      select(-Da, -Db) %>%
      round_df(3)
  }
  
  # function to apply to each resample
  calc_cohens_d <- function(split) {
    analysis(split) %>%
      group_by(session_id) %>%
      summarise(da = (mean(trial_latency[block_number == 5]) - mean(trial_latency[block_number == 2])) /
                  sqrt((sd(trial_latency[block_number == 5])^2 + sd(trial_latency[block_number == 2])^2)/2),
                db = (mean(trial_latency[block_number == 6]) - mean(trial_latency[block_number == 3])) /
                  sqrt((sd(trial_latency[block_number == 6])^2 + sd(trial_latency[block_number == 3])^2)/2)) %>%
      mutate(d = (da + db)/2) %>%
      ungroup() %>%
      select(-da, -db) %>%
      round_df(3)
  }
  
  # start timer
  start <- Sys.time()

  domains <- data_trimmed %>%
    distinct(domain) %>%
    pull(domain)
  
  # apply to each bootstrap
  data_D_bootstrapped_CIs <- 
    dplyr::bind_rows(
      lapply(seq_along(domains), function(i) {
        data_trimmed %>%
          filter(domain == domains[i]) %>%
          group_by(session_id) %>%
          bootstraps(times = nboots) %>%
          mutate(D_metrics = furrr::future_map(splits, calc_D)) %>%
          select(-splits) %>%
          unnest(D_metrics) %>%
          group_by(session_id) %>%
          dplyr::summarize(D_ci_lower = quantile(D, 0.025, na.rm = TRUE),
                           D_ci_upper = quantile(D, 0.975, na.rm = TRUE)) 
      })
    ) %>%
    mutate(D_sig = ifelse((D_ci_lower < 0 & D_ci_upper < 0) | (D_ci_lower > 0 & D_ci_upper > 0), TRUE, FALSE),
           D_ci_width = D_ci_upper - D_ci_lower) %>%
    round_df(3)
  
  data_d_bootstrapped_CIs <- 
    dplyr::bind_rows(
      lapply(seq_along(domains), function(i) {
        data_trimmed %>%
          filter(domain == domains[i]) %>%
          group_by(session_id) %>%
          bootstraps(times = nboots) %>%
          mutate(d_metrics = furrr::future_map(splits, calc_cohens_d)) %>%
          select(-splits) %>%
          unnest(d_metrics) %>%
          group_by(session_id) %>%
          dplyr::summarize(d_ci_lower = quantile(d, 0.025, na.rm = TRUE),
                           d_ci_upper = quantile(d, 0.975, na.rm = TRUE)) 
      })
    ) %>%
    mutate(d_sig = ifelse((d_ci_lower < 0 & d_ci_upper < 0) | (d_ci_lower > 0 & d_ci_upper > 0), TRUE, FALSE),
           d_ci_width = d_ci_upper - d_ci_lower) %>%
    round_df(3)
  
  data_estimates_with_CIs <- data_D_scores %>%
    full_join(data_D_bootstrapped_CIs, by = "session_id") %>%
    full_join(data_d_scores, by = "session_id") %>%
    full_join(data_d_bootstrapped_CIs, by = "session_id") %>%
    left_join(distinct(select(data_trimmed, session_id, domain), .keep_all = TRUE), by = "session_id") %>%
    select(session_id, domain, 
           D, D_ci_lower, D_ci_upper, D_sig, D_ci_width,
           d, d_ci_lower, d_ci_upper, d_sig, d_ci_width)
  
  # end timer
  end <- Sys.time()
  
  # calculate total time
  end - start

  # save to disk
  write_rds(data_estimates_with_CIs, "models/data_estimates_with_CIs.rds")
  
}

```

## Plots

```{r}

# plot
data_estimates_with_CIs %>%
  arrange(D) %>%
  mutate(ordered_id = row_number()) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = ordered_id, ymin = D_ci_lower, ymax = D_ci_upper, color = D_sig), 
                 alpha = 0.3) + 
  geom_point(aes(ordered_id, D, color = D_sig), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participants ranked by D score") +
  ylab("IAT D score")

```

```{r}

# plot
data_estimates_with_CIs %>%
  arrange(d) %>%
  mutate(ordered_id = row_number()) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = ordered_id, ymin = d_ci_lower, ymax = d_ci_upper, color = d_sig), 
                 alpha = 0.3) + 
  geom_point(aes(ordered_id, d, color = d_sig), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participants ranked by Cohen's d score") +
  ylab("IAT Cohen's d score")

```

```{r}

data_estimates_with_CIs %>%
  select(session_id, D, d) %>%
  gather(key, value, c(D, d)) %>%
  ggplot(aes(value, color = key)) +
  geom_density()

```


```{r fig.height=25, fig.width=8}

# separated by domains
data_estimates_with_CIs %>%
    group_by(domain) %>%
    arrange(D, .by_group = TRUE) %>%
    mutate(ordered_id = row_number()) %>%
  ungroup() %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = ordered_id, ymin = D_ci_lower, ymax = D_ci_upper, color = D_sig),
                 alpha = 0.3) +
  geom_point(aes(ordered_id, D, color = D_sig), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participant") +
  ylab("IAT D score") +
  facet_wrap(~domain, ncol = 5)

```

```{r fig.height=25, fig.width=8}

# separated by domains
data_estimates_with_CIs %>%
    group_by(domain) %>%
    arrange(d, .by_group = TRUE) %>%
    mutate(ordered_id = row_number()) %>%
  ungroup() %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_linerange(aes(x = ordered_id, ymin = d_ci_lower, ymax = d_ci_upper, color = d_sig),
                 alpha = 0.3) +
  geom_point(aes(ordered_id, d, color = d_sig), size = 0.5) +
  theme_classic() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_color_viridis_d(end = 0.6, direction = -1) +
  xlab("Participant") +
  ylab("IAT Cohen's d score") +
  facet_wrap(~domain, ncol = 5)

```

```{r}

data_estimates_with_CIs %>%
  summarize(median_half_ci_width = round(median(D_ci_width)/2, 2)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

ggplot(data_estimates_with_CIs, aes(D_ci_width)) +
  geom_density()

ggplot(data_estimates_with_CIs, aes(d_ci_width)) +
  geom_density()


ggplot(data_estimates_with_CIs, aes(D_ci_width, color = domain)) +
  geom_density() + 
  theme(legend.position = "none")

```


```{r}

ggplot(data_estimates_with_CIs, aes(D, D_ci_width)) +
  geom_point(alpha = 0.1)

ggplot(data_estimates_with_CIs, aes(d, d_ci_width)) +
  geom_point(alpha = 0.1)

```


```{r}

data_estimates_with_CIs %>%
  group_by(domain) %>%
  summarize(median_half_ci_width = round(median(D_ci_width)/2, 2)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

trimmed_emprirical_range <- data_estimates_with_CIs %>%
  dplyr::summarize(percentile_025 = quantile(D, 0.025, na.rm = TRUE),
                   percentile_975 = quantile(D, 0.975, na.rm = TRUE)) %>%
  mutate(range_empirical_95_percent = percentile_975 - percentile_025) 

```

## Proportion of participants demonstrating a significant bias

`r (1 - round(mean(data_estimates_with_CIs$D_sig), 3))*100`% of D scores are not signfiicant from zero. So, while they might appear to be relatively large (e.g., D = 0.5), their CI does not exclude zero. Put another way, if we treat the zero point as meaningful, we have insufficient evidence to say whether a given D score represents an IAT sig in `r (1 - round(mean(data_estimates_with_CIs$D_sig), 3))*100`% of cases in this large sample (`r nrow(data_estimates_with_CIs)` participants)).

## Estimation precision

Of course, while treating the zero point as meanining is common, it has been argued to problematic (i.e., by giving rise to false conclusions about the interpretations of the sig, see REF). While the previous analysis represents a useful illustration, it may be more meaningful to consider the precision of estimation of D scores, agnostic to an arbitrary cut-off point. 

The width of a D score Confidence intervals was found to be wide: M = `r round(mean(data_estimates_with_CIs$D_ci_width), 2)`, SD = `r round(sd(data_estimates_with_CIs$D_ci_width), 2)`, Median = `r round(median(data_estimates_with_CIs$D_ci_width), 2)`, MAD = `r round(mad(data_estimates_with_CIs$D_ci_width), 2)`. Results in the tables above suggest that it doesn't vary much by trial type or domain. As such, when an individual demonstrates a D score of X, we can more accurately say their D score lies in the range of X Â± a median of `r round(median(data_estimates_with_CIs$D_ci_width)/2, 2)`. 

While the minimum observed D scores was `r round(min(data_estimates_with_CIs$D), 2)` and max was `r round(max(data_estimates_with_CIs$D), 2)`, the outlier scores are clearly visible (see figure XXX). It is therefore likely to be more meaningful to note that 95% of D scores lie within the narrower range of `r round(as.numeric(trimmed_emprirical_range$percentile_025), 2)` to `r round(as.numeric(trimmed_emprirical_range$percentile_975), 2)`. It is useful to contextualize the precision of the estimation of a given IAT D score within this total observed ranged of D scores between participants. Specifically, the median CI width noted above represents `r round(median(data_estimates_with_CIs$D_ci_width)/as.numeric(trimmed_emprirical_range$range_empirical_95_percent)*100, 1)`% of the (95% trimmed) observed range of D scores. That is to say, the uncertainty around a given D score represents one third of the observed range of D scores: even knowing an individual's observed D score (e.g., moderately pro-white/anti-black), their 'true' D score may lie elsewhere on the range of possible values (e.g., from very pro-white/anti-black to very anti-white/pro-black). An individual IAT sig is therefore quite a poor measure for individual use.

This can also be examined another way by posing the question 'what proportion of D scores can you tell apart from one another?' That is, is the probability that a given D score lies outside the CI of all the other D scores' CIs. For simplicity of implementation, this analysis compares all D scores against all confidence intervals. It is therefore slightly biased by comparing a D score against its own CI as well as all others. However, given the large number of comparisons (i.e., sample size: i.e., `r nrow(data_estimates_with_CIs)` total D scores) this bias is very slight. 95% CIs of this probability value are bootstrapped via case removal and the percentile method using 1000 resamples. The median bootstrapped probability is reported as the estimate for the sake of robustness.

```{r}

# bootstrapping has a long execution time, so load saved values if they've already been calculated
if(file.exists("models/data_pairwise_comparisons.rds")) {
  
  data_pairwise_comparisons <- read_rds("models/data_pairwise_comparisons.rds")
  
} else {
  
  boots <- data_estimates_with_CIs %>%
    group_by(domain) %>%
    bootstraps(times = 1000)
  
  # helper function to apply workflow to each resample
  helper_function <- function(split) {
    
    estimate <- analysis(split)$D
    ci_lower <- analysis(split)$D_ci_lower
    ci_upper <- analysis(split)$D_ci_upper
    
    n_estimate <- length(estimate)
    n_ci_lower <- length(ci_lower)
    n_ci_upper <- length(ci_upper)
    
    r_estimate <- sum(rank(c(estimate, ci_lower))[1:n_estimate])
    r_ci_upper <- sum(rank(c(ci_upper, estimate))[1:n_ci_upper])
    
    prob_estimate_inferior_to_ci_lower <- 1 - (r_estimate / n_estimate - (n_estimate + 1) / 2) / n_ci_lower
    prob_estimate_superior_to_ci_upper <- 1 - (r_ci_upper / n_ci_upper - (n_ci_upper + 1) / 2) / n_estimate
    
    percent_estimates_inside_cis <- 1 - (prob_estimate_inferior_to_ci_lower + prob_estimate_superior_to_ci_upper)
    
    return(percent_estimates_inside_cis)
    
  }
  
  # apply to each bootstrap
  boot_probabilities <- boots %>% 
    mutate(percent_estimates_inside_cis = furrr::future_map(splits, helper_function)) %>% 
    unnest(percent_estimates_inside_cis) %>%
    select(-splits)
  
  # find CIs using percentile method
  data_pairwise_comparisons <- boot_probabilities %>% 
    summarize(median   = quantile(percent_estimates_inside_cis, 0.500),
              ci_lower = quantile(percent_estimates_inside_cis, 0.025),
              ci_upper = quantile(percent_estimates_inside_cis, 0.975)) %>%
    round_df(3)
  
  
  # save to disk
  write_rds(data_pairwise_comparisons, "models/data_pairwise_comparisons.rds")
  
}
  
```
  
It is not possible to differentiate between two randomly selected D scores in `r round(data_pairwise_comparisons$median*100, 1)`% (95% CI [`r round(data_pairwise_comparisons$ci_lower*100, 1)`, `r round(data_pairwise_comparisons$ci_upper*100, 1)`]) of cases. This provides additional evidence that IRAP's individual level precision and therefore clinical utility is low.


